%%%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[10pt, sigplan]{acmart}
%%\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,10pt,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,10pt,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,10pt,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[sigplan,10pt]{acmart}\settopmatter{}


%% Conference information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
%\acmConference[PL'17]{ACM SIGPLAN Conference on Programming Languages}{January 01--03, 2017}{New York, NY, USA}
%\acmYear{2017}
%\acmISBN{} % \acmISBN{978-x-xxxx-xxxx-x/YY/MM}
%\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
%\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2017}           %% If different from \acmYear

%% Bibliography style

%\bibliographystyle{ACM-Reference-Format}

%% Citation style
%\citestyle{acmauthoryear}  %% For author/year citations
\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from traditional SIGPLAN
%% proceedings format to PACMPL format must update the
%% '\documentclass' and topmatter commands above; see
%% 'acmart-pacmpl-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{ifthen}
\usepackage{pgfplots}
\usepackage{listings}
\usepackage{multirow}
\usepgfplotslibrary{statistics}
\usepackage{dblfloatfix} %enable fig at bottom of page
%\input{macros.tex}

\usepackage{xcolor}
\newcommand{\todo}[1]{\color{orange}\fbox{\bfseries\sffamily\scriptsize TODO:}{\sf\small$\blacktriangleright$\textit{#1}$\blacktriangleleft$}\color{black}}
\newcommand{\sk}[1]{\color{blue}\fbox{\bfseries\sffamily\scriptsize Sophie:}{\sf\small$\blacktriangleright$\textit{#1}$\blacktriangleleft$}\color{black}}
\newcommand{\cba}[1]{\color{purple}\fbox{\bfseries\sffamily\scriptsize Clement:}{\sf\small$\blacktriangleright$\textit{#1}$\blacktriangleleft$}\color{black}}
%\newcommand*{rotatebox{75}}
\input{macros}

\begin{document}

%% Title information
\title[Garbage Collection Evaluation Infrastructure]{Garbage Collection Evaluation Infrastructure\\ for the Cog VM}

%% Author with single affiliation.
\author{Sophie Kaleba}
                                        %% can be repeated if necessary
\affiliation{
  %\position{Position1}
  \department{Master student}              %% \department is recommended
  \institution{Universit\'e de Lille 1}            %% \institution is required
 % \streetaddress{Street1 Address1}
  \city{Lille}
  %\state{France}
  %\postcode{Post-Code1}
  \country{France}                    %% \country is recommended
}
\email{sophie.kaleba@etudiant.univ-lille1.fr}          %% \email is recommended

%% Author with two affiliations and emails.
\author{Cl\'ement B\'era}
\affiliation{
  % \position{}
	\department{Software Languages Lab}              %% \department is recommended
	\institution{Vrije Universiteit Brussel}            %% \institution is required
	\city{Brussel}
  % \state{}
  % \postcode{}
	\country{Belgium}                    %% \country is recommended
}
\email{clement.bera@vub.ac.be}          %% \email is recommended

%% Author with two affiliations and emails.
\author{Eliot Miranda}
\affiliation{
   \position{Virtual Machine Architect}
	%\department{Virtual Machine architect}              %% \department is recommended
	\institution{Feenk}            %% \institution is required
	\city{San Francisco}
  % \state{}
  % \postcode{}
	\country{California}                    %% \country is recommended
}
\email{eliot.miranda@gmail.com}          %% \email is recommended

\begin{abstract}
One of the next step to improve the Cog virtual machine, the default virtual machine for multiple programming languages such as Pharo, Squeak and Newspeak, is to decrease the garbage collection pause time. A benchmarking infrastructure and reference garbage collection algorithm implementations are required to evaluate the performance of a new algorithm and compare it. Cog features a Mark-Compact algorithm, used in production, and we introduced a Mark-Sweep algorithm as the two reference algorithms. Benchmarks are built using two different approaches. Firstly, we turned code from memory intensive deployed applications into benchmarks to simulate real-world applications. Secondly, we built a configurable benchmark which simulates an application with different heap properties to be able to stress specific aspects of the memory management. We then evaluated the two reference algorithms on the infrastructure built to have reference benchmark results.
\end{abstract}

\keywords{Benchmark, Garbage Collector, Virtual machine, Managed runtime}  %% \keywords are mandatory in final camera-ready submission

\maketitle

\section{Introduction}
\label{sec:intro}

The Cog virtual machine (VM), the default VM for multiple programming languages such Pharo \cite{PharoByExample}, Squeak \cite{SqueakByExample} and Newspeak \cite{NewspeakOopsla}, currently features in production a stop-the-world Mark-Compact algorithm as the full garbage collector (GC) algorithm. The algorithm has a high throughput but a high pause time during which the application is not responsive (the GC interrupts the application for multiple seconds on modern Macbooks starting from multiple Gbs heaps). For interactive applications, a new algorithm is required with a smaller pause time.

Building and tuning a new GC algorithm requires to evaluate its behavior. A benchmarking infrastructure is required to do so. To build GC benchmarks, we took two approaches. 

Firstly, we contacted multiple companies using the Cog VM in production on memory intensive application (>1Gb heaps) and built benchmarks out of their deployed application. As an example, we will discuss in the paper in Section \ref{sec:mooseBench} the Moose benchmarks implemented in collaboration with Feenk\footnote{https://feenk.com/}. Part of the business of Feenk consists in analysing software written in multiple programming languages using the open-source framework Moose~\cite{MoosePaper1,MooseBook1}. Using Moose, the application parses the software to analyse into a model, performs analysis on it and lastly releases the model. This behavior was turned into three benchmarks, growing, accessing and shrinking the heap. 

Secondly, we implemented a configurable benchmark: the idea is to set-up specific heap structures that will stress the garbage collector. A set of specific options is available to the user to tune the type of allocated objects and some of their features. These options can be associated with different heap states: growing, stable or shrinking, like in our other approach. \todo{fin a ref}
\sk{add the case we benchmarked with this approach}
This implementation and the use-cases are detailed in Section \ref{sec:confBench}

To evaluate a new algorithm, we also need to compare it to reference implementations. The Cog VM features a Mark-Compact algorithm in production. The compaction phase is quite specific since it moves objects in memory and spend time updating references. In addition to this algorithm, we implemented a Mark-Sweep, which does not move objects in memory and is quicker to perform. Section \ref{sec:ref} details the two implementations.

We evaluated both algorithm on the benchmarks we built to provide results of the reference algorithms on the infrastructure built.

We conclude the paper by discussing some related work, the GC benchmarks suites for Java and the ACDC benchmarks, and some future work.

\section{Building Benchmarks}
\label{sec:bench}
\sk{isn't it weird to introduce the built benchmarks without explaining *why* they were chosen? Like, why growing, stable, shrinking is significant regarding garbage collection? same for my 3 benchs, why would I chose those 3 bench strategies? I know it is kind of explained in the result section but maybe it is a bit late}

To build GC benchmarks, we took two approaches. We asked companies for their deployed memory intensive applications and turned them into benchmarks. Multiple benchmarks in this category include closed-source code. In Section \ref{sec:mooseBench}, we describe how we turned a specific application into a benchmark with the help of the company (this specific benchmark does not include closed-source code). Then, to stress the GC on specific aspects, we built a configurable benchmark that based on the configuration emulates heaps with different properties. 

\subsection{Moose benchmark}
\label{sec:mooseBench}

Part of the business of Feenk consists in analyzing softwares. To do so, the application parses a mse file into a model, performs some analysis, and then releases the model and generates analysis results. For business, the application analyzes closed-sourced application of companies. To build an open-source benchmark, we analyse open-source applications instead of closed-source ones.

We built four different benchmarks that falls into three categories: growing, stable and shrinking heaps. The benchmarks were built on top of the stable Moose image\footnote{http://www.moosetechnology.org/}.

\paragraph{Growing heap.} The growing heap benchmarks increase the heap size as they are performed, mainly creating objects. We built two benchmarks:
\begin{itemize}
	\item \emph{LoadFromMSE}: parses a mse file into a moose model, effectively loading a graph of object. This is the only benchmark not taking a moose model as a parameter.
	\item \emph{ExpandProperties}: computes all interesting properties of the model, discard properties rarely used or consuming too much memory, cache common properties not using too much memory. It effectively loads a graph of object connected in many points to the original graph. 
\end{itemize}

\paragraph{Stable heap.} The stable heap benchmarks perform random memory accesses in a heap which size remains approximately the same. We built one benchmark, \emph{ExpandPropertiesWithCache}, which computes all interesting properties of the model, common properties with low memory footprint are already cached and other properties are discarded once computed.

\paragraph{Shrinking heap.} The shrinking heap benchmark frees part of the heap and performs a GC. We built only one benchmark, \emph{Release}, which removes references to the graph of objects and performs three garbage collections.

\subsection{Configurable benchmark}
\label{sec:confBench}

Running a new memory management strategy on real-world applications can give a first insight about its performance. Being able to configure a specific set-up, personalized environment, can allow to locally \sk{rephrase} test specific behaviors of the new implementation.
This configurable benchmarking tool has been implemented to emulate specific heap structures and states to potentially trigger edge cases: the user has available a set of twelve options which can be sorted in two categories:

\paragraph{Heap structure.} A first set of options allows the user to modify the heap structure, i.e.\ the amount and sizes of objects to allocate and the possible interactions with these objects. \todo{maybe structure is not actually a good term}

\begin{itemize}
\item \emph{wantedAllocatedSize:} the cumulated size (in Kb) of objects that will be allocated
\item \emph{minSize and maxSize:} the minimum and maximum size of objects
\item \emph{dataObjects, weakStructures, classes, compiledMethods:} the ratio for each of these object types.
\item \emph{accessObjects:} if set to True, access half of the objects allocated on the heap 
\item \emph{readOnly:} if set to False, modify half of the objects allocated on the heap upon accessing  
\end{itemize}

\paragraph{Heap state.} The heap state can also be tuned. Once objects have been allocated on the heap according to the selected options, it is possible whether to allocate more objects (growing heap), to release objects that have been previously allocated (shrinking heap), or to stay in the current state and potentially accessing and modifying objects that have been previously allocated (stable heap). These options echoes the 3 benchmarks depicted in Section \ref{sec:mooseBench}\\

We build three benchmarks out of the potential configurations:
\begin{itemize}
\item \emph{FragmentedHeap:} First, objects are allocated one the heap, and some of them are released, leading to a potential memory fragmentation. Then, new objets with a minimum size greater than the new free chunks sizes are allocated.
\item \emph{LargeObjects:} Only large objets with a size greater than 0xFFFF \sk{put real size in Kb here} are allocated on the heap.
\item \emph{Data0bjects:} The majority of objects that will be allocated on the heap will be Data objects, which only contains integer of a specific size (in this benchmark, one byte)
\end{itemize}


\section{Reference Implementations}
\label{sec:ref}

To evaluate new full GC algorithms, we need to compare it against existing algorithms. Two of the most common GC algorithms are Mark-Compact and Mark-Sweep. We describe here the current Mark-Compact implementation, which has been in production for the past few years, and an implementation of Mark-Sweep we introduced as a reference earlier this year. 

\subsection{Mark-Compact}

Cog's Mark-Compact algorithm, named \emph{SpurPlanningCompactor}, implements the classic planning compaction algorithm for Spur.  It uses the fact that there is room for a forwarding pointer in all objects to store the eventual position of an object in the first field. It therefore first locates a large free chunk, or eden or a memory segment, to use as the savedFirstFieldsSpace, which it uses to store the first fields of objects that will be compacted. It then makes at least three passes through the heap.

The first pass plans where live movable objects will go, copying their first field to the next slot in savedFirstFieldsSpace, and setting their forwarding pointer to point to their eventual location (see planCompactSavingForwarders). The second pass updates all pointers in live pointer objects to point to objects' final destinations, including the fields in savedFirstFieldsSpace (see updatePointers and updatePointersInMobileObjects). The third pass moves objects to their final positions, unmarking objects, and restoring saved first fields as it does so (see copyAndUnmark: and copyAndUnmarkMobileObjects). If the forwarding fields of live objects in the to-be-moved portion of the entire heap won't fit in savedFirstFieldsSpace, then additional passes can be made until the entire heap has been compacted.  When snapshotting multiple passes are made, but when doing a normal GC only one pass is made.

Each pass uses a three finger algorithm, a simple extension of the classic two finger algorithm with an extra finger used to identify the lowest pinned object between the to and from fingers.  Objects are moved down, starting at the first free object or chunk, provided that they fit below the lowest pinned object above the to finger.  When an object won't fit the to finger is moved above the pinned object and the third finger is reset to the next pinned object below the from finger, if any.

\subsection{Mark-Sweep}

Cog's Mark-Compact algorithm, named \emph{SpurSweeper}, is a sweep-only algorithm, setting the compactor to SpurSweeper effectively changes the fullGC to a mark-sweep non-moving algorithm. It iterates over all old space in linear order. Each time an unmarked object or a free chunk is met, it coalesces it with subsequent ones and updates the free lists with a new larger free chunk. 

\subsection{Snapshot discussion}

On top of the Cog VM, snapshots can be performed to persist the given state of the heap. Since snapshots are persisted, they should use a low amount of memory to avoid using too many bytes on disk. Non compacting algorithms, such as the Mark-Sweep, or algorithms compacting only part of the heap, such as garbage first~\cite{G1}, are a problem in this context. The heap, when collected by the Mark-Sweep, has a lot of free chunks when the heap size varies a lot leading to a large snapshot. However, non compacting have also interesting advantages (lower garbage collection pause time for example). 

To be able to have a garbage collector with a low pause time and still be able to perform efficiently snapshots, we design a hybrid compactor solution. The VM tells the compactor if the garbage collection is performed for snapshot or not, and the compactor chooses to apply one algorithm or the other to have low compaction pause time or to snapshot with a low memory footprint.

\section{Reference Results}
\label{sec:valid}

We evaluated both reference algorithms on the benchmarks built.
\sk{shouldn't we talk about our set-ups? or maybe not because the 2 benchs were run on different computers ?}

\subsection{Moose Benchmarks}


%
\begin{figure*}[thb]
	\centering
    
    \begin{subfigure}[b]{.48\textwidth}
	\includegraphics[width=\linewidth]{figures/load} 
	\caption{Load\vspace{0.5cm}}
   	\end{subfigure}\hspace{0.03\textwidth}% 
   	\begin{subfigure}[b]{.48\textwidth}
	\includegraphics[width=\linewidth]{figures/prop} 
	\caption{ExpandProperties\vspace{0.5cm}}
   	\end{subfigure}	
	\begin{subfigure}[b]{.48\textwidth}
	\includegraphics[width=\linewidth]{figures/propCache} 
	\caption{ExpandPropertiesWithCache}
	\end{subfigure\hspace{0.03\textwidth}}%
	   	\begin{subfigure}[b]{.48\textwidth}
	\includegraphics[width=\linewidth]{figures/release} 
	\caption{release}
   	\end{subfigure}

   	   	    	
\caption{Moose benchmark results piled up (in ms).}
\label{MooseRes1}
\end{figure*}

\begin{figure*}[thb]
\begin{tabular}{|c|c|c|c|c|c|c|c|}
   \hline
  & & Total   & Scavenge  & Total Full  & Compaction  & Initial  & Final \\
 Benchmark & &  Exec. time &  time & GC time &  time &  Heap size & Heap size \\
  & &  (ms) &  (ms) & (ms) &  (ms) &  (Mb) &  (Mb) \\
   \hline
   \multirow{2}{*}{Load} & Sweep 	& 159821 $\pm$ 2218 &	73599 $\pm$ 1281 	& 10863 $\pm$ 726 	&1398 $\pm$ 091 	& 193 $\pm$ 0 	& 959 $\pm$ 9.69 \\
    				    & Compact 	& 162305 $\pm$ 2790 &	72150 $\pm$ 0782 	& 14091 $\pm$ 972 	&4661 $\pm$ 289 	& 193 $\pm$ 0 	& 909 $\pm$ 9.69 \\
   \hline
   \multirow{2}{*}{ExpandProperties} 	& Sweep 		& 232940 $\pm$ 01287 &	55497 $\pm$ 0238 	& 12240 $\pm$ 285 	&1178 $\pm$ 6.66 	& 959 $\pm$ 9.69 & 1888 $\pm$ 0 \\
    				    			& Compact 	& 183314 $\pm$ 13090 &	53165 $\pm$ 2343 	& 12496 $\pm$ 197 	&3775 $\pm$ 0108 	& 909 $\pm$ 9.69 & 1938 $\pm$ 0 \\
   \hline
   ExpandProperties 		& Sweep 		& 510013 $\pm$ 05812 &	64234 $\pm$ 0823 	& 23225 $\pm$ 100 	&2011 $\pm$ 10 	& 1888 $\pm$ 0 	& 1888 $\pm$ 0 \\
   WithCache  			& Compact 	& 408169 $\pm$ 20444 &	64492 $\pm$ 1234 	& 0 $\pm$ 0 		&0 $\pm$ 0 		& 1938 $\pm$ 0 		& 1938 $\pm$ 0 \\
   \hline
   \multirow{2}{*}{Release} 	& Sweep 		& 0606 $\pm$ 16 		&	0 $\pm$ 0 	& 0605 $\pm$ 16 	&316 $\pm$ 12 		& 1888 $\pm$ 0 	& 534 $\pm$ 58 \\
    				    		& Compact 	& 1092 $\pm$ 02 	&	0 $\pm$ 0 	& 1092 $\pm$ 01 		&806 $\pm$ 03 			& 1938 $\pm$ 0 		& 193 $\pm$ 00 \\
   \hline
\end{tabular} 
\caption{Moose benchmark results with standard errors.}
\label{MooseRes2}
\end{figure*}

\sk{my benchs relies on the same data (the 2 algorithms) + same explanation of the time (full gc and so on). I would move the shared part in the intro of Section 4 and only keep here the part specific to the Moose bench}
We evaluated the Moose benchmarks using the reference Mark-Compact and Mark-Sweep algorithms and by analysing the wildfly code base\footnote{http://www.wildfly.org/}. The results are shown in Figure \ref{MooseRes1} and \ref{MooseRes2}. In the results, the compaction time includes the time spent in the compact phase in the Mark-Compact and in the sweep phase in the Mark-Sweep. Total full GC time is the total time spent in full GCs, including the compaction time. Scavenge time is the time spent in scavenges, \emph{i.e.,} garbage collection of young objects only. Total execution time is the total execution time, including scavenge and full GC time.

\paragraph{Load.} The Load benchmark grows the memory by 750 Mb with Sweep and by 700 Mb with Compact. Compaction time is smaller for the sweep algorithm, making the benchmark a little bit faster in the Mark-Sweep, which is to be expected since Sweep does not need to waste time updating pointers because objects are not moved in memory.

\paragraph{ExpandProperties.} The ExpandProperties benchmark grows the memory by around 1 Gb. Compaction time is a little bit better with the sweep algorithm as for the previous benchmark, but execution time is much worse. We believe this is due to worse locality of objects. 

\paragraph{ExpandPropertiesWithCache.} The ExpandProperties WithCache benchmark does not increase the overall memory size. Since the compacting GC has compacted the heap before starting the benchmark, there is enough room to allocate the large objects (large objects are allocated directly in old space) required by the benchmark and there is no need for a full GC during the benchmark. However, with the sweep algorithm, there are multiple small available memory chunks which are not big enough to hold all the requested large objects, so a full GC is required. In addition, execution time is worse, as for the previous benchmark.

\paragraph{Release.} The Release benchmark releases all the graphs. With the compacting GC, memory goes back to its original size. With Sweep, multiple memory segments are left with a few live objects, forbidding the OS to reclaim those segments. Hence, the memory decreases drastically, but not to the original size. Compaction time is much faster with Sweep, but as we have discussed, for less efficiency.

\subsection{Configurable benchmark}

We evaluated the configurable benchmarks using the reference Mark-Compact and Mark-Sweep algorithms and by generating specific heap configurations. 
The resulats are shown in Figure \ref{ConfigRes1} and \ref{ConfigRes2}

\paragraph{FragmentedHeap.} 
\begin{itemize}
\item impact on memory size and numbers of segments
\item impact on execution time 
\end{itemize}
\paragraph{LargeObjects.}
\begin{itemize}
\item impact on memory size 
\item impact on number of scavenges and number of full GC's
\end{itemize}
\paragraph{Data0bjects.}
\begin{itemize}
\item impact on memory size
\item impact on overall time (full gc, compaction time)
\end{itemize}

\section{Related and Future Work}

\paragraph{GC benchmark suite.} The DaCapo benchmarks \cite{DacapoBench} is a standard benchmarking suite designed for Java applications. It gathers several open-source benchmarks and run them against specific metrics to assess their potential performance \sk{significance towards} on different aspects of the Java language, typically its memory management and virtual machines.
With regard to memory management, the eleven benchmarks selected in 2006 (and updated later in 2009, to add three more benchmarks \ref{dacapowebsite} \sk{add ref in bib}) are assessed according to their number of allocated objects and their average size and their maximum number of live objects. The nursery (young space) survival rate is also taken into account, as well as the heap structure through object lifetime behavior.
These criteria were chosen \sk{rephrase} as significant to asses the performance of memory management. 

%The part dedicated to memory management focuses on three metrics: allocation demographics, lifetime and live objects. The benchmarks used shows a rich and diverse object lifetime behaviors. --> dacapo allocates more objets, 3 time the live size of SPEC.
%Heap volume, Heap objects, Mean object size, nursery survival rate.
%
%measure object size demographics (size demographics for each benchmark)
%--> live and allocation time serie analysis
%plot heap composition (allocations and pointer mutations)
%--> richer lifetime behavior 


\paragraph{ACDC-JS.}
ACDC-JS \cite{ACDCJS} is a configurable \\benchmarking tool designed to specifically measure and stress allocation and deallocation time in the JavaScript memory management model. 
It builds a heap model based on the behavior of real-time applications and aims at providing significant insights on the JavaScript memory management that cannnot be obtained from standard benchmarking suites. The value of specific heap options can be set by the user. 
Their study shows the negative impact of object liveness (time between allocation and last access, leading to the objects to be in different spaces in the heap) and deallocation delay on allocation time. It also stresses the trade-offs between memory consumption and allocation latency.

\paragraph{Future work: more benchmarks.}
We described two approaches to bench new memory management strategies: the Moose benchmark, based on a real-world application and the configurable benchmarking tool, highlighting edge cases by stressing the garbage collector on specific aspects. It would be nice to add some benchmarks based on the standard DaCapo benchmark suite so we could assess the performance of our memory management strategies via a reputed benchmarking suite.

\paragraph{Future work: stress GC tests.}
We can stress the garbage collector by running the configurable benchmarking tool with specific sets of options to test the memory startegy in edge cases. By increasing the amount of edge-cases benchmarks led on the garbage collector, we could infer a serie of tests to secure the garbage collector behavior in these situations.

\paragraph{Future work: more options.}
The number of options available in the configurable tool is still limited at the moment. We can take advantage of the ACDC-JS implementation, as well as some metrics used in the DaCapo paper, to get a higher number of options and improve the data collected during the benchmark.

%% Bibliography
\bibliographystyle{alpha}
\bibliography{sista}


\end{document}
